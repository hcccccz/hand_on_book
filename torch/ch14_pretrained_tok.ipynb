{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb6dd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b57971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imdb_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "split = imdb_dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "imdb_train_set, imdb_valid_set = split['train'], split['test']\n",
    "imdb_test_set = imdb_dataset['test']\n",
    "\n",
    "train_reviews = [review['text'] for review in imdb_train_set]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa442c4b",
   "metadata": {},
   "source": [
    "### GPT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65d3f38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[29391, 35030, 1690, 423, 257, 1688, 8046, 13, 1119, 1690, 1282, 503, 2045, 588, 257, 2646, 4676, 373, 2391, 4624, 319, 262, 3800, 357, 16678, 355, 366, 24732, 10584, 11074, 35727, 18348, 316, 338, 4571, 7622, 262, 2646, 6776, 11, 543, 318, 2592, 2408, 1201, 262, 4286, 4438, 683, 645, 1103, 4427, 13, 7831, 11, 340, 338, 3621, 284, 804, 379, 329, 644, 340, 318, 13, 383, 16585, 1022, 3899, 327, 5718, 290, 12803, 29030, 303, 318, 2407, 10457, 13, 383, 17262, 286, 511, 2776, 389, 6452, 13, 327, 5718, 318, 9623, 355, 1464, 11, 290, 29030, 303, 3011, 530, 286, 465, 1178, 8395, 284, 1107, 719, 29847, 1671, 1220, 6927, 1671, 11037, 40, 22127, 326, 314, 1053, 1239, 1775, 314, 430, 32325, 338, 711, 11, 475, 314, 3285, 326, 9180, 4332, 261, 9659, 338, 16711, 318, 17074, 13, 383, 4226, 318, 8131, 47370, 11, 290, 7622, 345, 25260, 13, 366, 20148, 46670, 1, 318, 281, 36005, 17774, 2646, 11, 290, 318, 7151, 329, 3016, 477, 3296, 286, 3800, 290, 3159, 29847, 1671, 1220, 6927, 1671, 11037, 22, 13, 19, 503, 286, 838], [6, 464, 36595, 6, 373, 257, 7932, 3807, 546, 262, 1218, 8395, 1204, 6622, 329, 514, 290, 635, 7584, 281, 7016, 1807, 625, 262, 5386, 11, 1642, 606, 6537, 326, 534, 10625, 460, 1282, 2081, 13, 1002, 345, 6151, 705, 16676, 262, 18711, 3256, 705, 464, 36595, 6, 318, 262, 3807, 329, 345, 3228, 632, 338, 262, 1254, 922, 3807, 286, 262, 614, 290, 340, 318, 262, 2818, 3807, 329, 477, 9337, 13, 705, 464, 36595, 6, 7127, 257, 1688, 1363, 1057, 0], [11380, 11, 4360, 857, 326, 787, 428, 257, 922, 3807, 30, 4053, 11, 1662, 1107, 11, 259, 616, 4459, 13, 8117, 2125, 470, 257, 2187, 1256, 284, 4313, 340, 13, 72, 1043, 340, 845, 3105, 11, 1513, 6819, 11, 259, 1109, 13, 270, 338, 635, 20039, 2495, 881, 832, 290, 832, 13, 17618, 530, 290, 734, 547, 6454, 20039, 11, 4360, 407, 355, 881, 13, 72, 635, 2936, 428, 3807, 373, 2407, 1413, 88, 379, 1661, 11, 4758, 1312, 1422, 470, 1107, 892, 11414, 428, 2168, 290, 262, 2095, 13, 19139, 18350, 20342, 5341, 262, 1388, 2089, 3516, 287, 428, 25168, 13, 258, 338, 257, 7709, 1576, 8674, 11, 4360, 1312, 2936, 339, 2826, 465, 2095, 1165, 625, 262, 1353, 13, 72, 4724, 326, 4197, 351, 262, 8216, 286, 262, 3807, 11, 4758, 561, 423, 587, 1049, 611, 1312, 550, 8288, 262, 3807, 13, 9541, 11, 8117, 547, 617, 2495, 2089, 530, 9493, 364, 13, 3163, 77, 727, 569, 418, 29680, 5860, 287, 262, 3670, 2597, 11, 4360, 318, 1813, 1310, 284, 670, 351, 287, 428, 3807, 13, 1169, 2095, 468, 407, 1107, 12572, 11, 292, 1312, 550, 10719, 13, 1219, 880, 13, 5661, 318, 655, 616, 4459, 13, 1092, 1014, 11, 1640, 502, 11, 4514, 428, 3807, 318, 407, 450, 893, 7617, 11, 270, 318, 2495, 2089, 13, 1820, 3015, 329, 3801, 805, 6711, 25, 513, 13, 20, 14, 20]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True, max_length=500)\n",
    "gpt_encoding #attributes: (input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15df2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29391, 35030, 1690, 423, 257, 1688, 8046, 13, 1119, 1690]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stage adaptations often have a major fault. They often'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_token_ids = gpt_encoding['input_ids'][0][:10]\n",
    "print(gpt_token_ids)\n",
    "gpt2_tokenizer.decode(gpt_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36061b8c",
   "metadata": {},
   "source": [
    "### BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "985e637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True, truncation=True, max_lenth=500, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "019bccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([ 2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,  2411])\n"
     ]
    }
   ],
   "source": [
    "print(bert_encoding[\"input_ids\"][0][:10])\n",
    "print(bert_encoding[\"attention_mask\"][0][:10])\n",
    "\n",
    "\"\"\"Notice that each token ID sequence starts with token 101 ([CLS]), and ends\n",
    "with token 102 ([SEP]) (ignoring padding tokens).\"\"\"\n",
    "\n",
    "\"\"\"drop special token [CLS] and [SEP]\"\"\"\n",
    "bert_encoding_1 = bert_tokenizer(train_reviews[:3],\n",
    "                                padding=True,\n",
    "                                truncation=True,\n",
    "                                max_lenth=500,\n",
    "                                return_tensors=\"pt\",\n",
    "                                add_special_tokens=False)\n",
    "\n",
    "print(bert_encoding_1[\"input_ids\"][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae42ae0",
   "metadata": {},
   "source": [
    "### Wrap own tokenizer to have same api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25d00dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
    "\"\"\"分词前先用空格分词\"\"\"\n",
    "# bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "\"\"\"把所有空格替换成Ġ\"\"\"\n",
    "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel()\n",
    "special_tokens = ['<pad>', \"<unk>\"]\n",
    "bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000, special_tokens=special_tokens)\n",
    "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
    "bpe_tokenizer.train_from_iterator(iterator=train_reviews, trainer=bpe_trainer)\n",
    "\n",
    "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "bpe_tokenizer.enable_truncation(max_length=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8f84a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[196, 499, 460, 40, 351, 862, 159, 59, 146, 264]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=bpe_tokenizer)\n",
    "\n",
    "hf_encodings = hf_tokenizer(train_reviews[:3], padding=True)\n",
    "\n",
    "hf_encodings['input_ids'][0][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

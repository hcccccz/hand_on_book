{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c0dc49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11a2088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_shakespeare_text():\n",
    "    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    url = \"https://homl.info/shakespeare\"\n",
    "    urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a259f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = download_shakespeare_text()\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41777759",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(shakespeare_text))\n",
    "\"\".join(vocab)\n",
    "\n",
    "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
    "id_to_char = {index:char for char, index  in char_to_id.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8849655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, world!'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
    "\n",
    "def decode_text(char_ids):\n",
    "    return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])\n",
    "encoded = encode_text(\"Hello, world!\")\n",
    "\n",
    "decode_text(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8a9226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text: str, window_length: int):\n",
    "        self.encoded_text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.window_length\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        end = index + self.window_length\n",
    "        window = self.encoded_text[index: end]\n",
    "        target = self.encoded_text[index+1: end+1]\n",
    "        return window, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27de6083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d22f51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "batch_size = 512\n",
    "vocab_size = len(vocab) #65\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
    "valid_set = CharDataset(shakespeare_text[1_000_000: 1_060_000], window_length)\n",
    "test_set = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "\n",
    "x,y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42bf827a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 65, 50])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_layers: int = 2, embed_dim: int = 10, hidden_dim: int = 128, dropout:float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "                        input_size=embed_dim,\n",
    "                        hidden_size=hidden_dim,\n",
    "                        num_layers=n_layers,\n",
    "                        batch_first=True,\n",
    "                        dropout=dropout\n",
    "                              )\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X shape: (B, seq_len)\n",
    "        embeddings shapes: (B, seq_len, D)\n",
    "        rnn_out: (B, seq_len, H)\n",
    "        out: (B, seq_len, vocab_size)\n",
    "        out: (B, vocab_size, seq_len) =(B, C, d_1)\n",
    "\n",
    "        \"\"\"\n",
    "        embeddings = self.embed(X)\n",
    "        rnn_out, hidden_state = self.gru(embeddings)\n",
    "        out = self.output(rnn_out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "\n",
    "model = ShakespeareModel(vocab_size=vocab_size)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd924c30",
   "metadata": {},
   "source": [
    "#### Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48517286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "text = \"inference exampleswr\"\n",
    "encoded_text = encode_text(text).unsqueeze(dim=0) #(18) => (1, 18)\n",
    "with torch.no_grad():\n",
    "    y_logits = model(encoded_text) # (B, C, d_1)\n",
    "    \"\"\"\n",
    "    第0个batch, 所有vocab_size, 最后一个seq_len的索引\n",
    "    \"\"\"\n",
    "    prediced_char_id = y_logits[0,:, -1].argmax().item()\n",
    "    print(prediced_char_id)\n",
    "    predicted_char = id_to_char[prediced_char_id]\n",
    "    print(predicted_char)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "639c5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.tensor([[0.5, 0.4, 0.1]])\n",
    "\n",
    "samples = torch.multinomial(probs,replacement=True,num_samples=8)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70313118",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Temperature: <1 模型更贪心， 高概率的更高 低概率的更低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(model: nn.Module, text: str, temperature:float = 1):\n",
    "    \"predict next character based on model\"\n",
    "    encoded_text = encode_text(text).unsqueeze(dim=0) #(seq_len) => (1,seq_len)\n",
    "    with torch.no_grad():\n",
    "        y_logits = model(encoded_text)\n",
    "        \"\"\"y_logits (B, vocab_size, seq_len)\n",
    "            y_logits[0,:, -1] (vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        y_prob = F.softmax(y_logits[0, :, -1] / temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(y_prob, num_samples=1, replacement=False).item()\n",
    "        return id_to_char[predicted_char_id]\n",
    "\n",
    "def extend_text(model, text, n_chars=80, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa7c9f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be nor no to ba3\\n$sWgvp:'r\\nPUJa:x.ZH3j.?vrlcbXsDcddC&'$ZcjucE3S AEcHOpQXiszKb'I-UrGBlw'gImmwL-j\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend_text(model, \"to be nor no to ba\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
